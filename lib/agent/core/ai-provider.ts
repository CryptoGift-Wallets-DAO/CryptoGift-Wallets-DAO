/**
 * AI Provider - GPT-5 Official Implementation (September 2025)
 * 
 * ‚ö†Ô∏è CRITICAL: GPT-5 was officially released August 7, 2025
 * Reference: https://openai.com/index/introducing-gpt-5/
 * 
 * Implements GPT-5 September 2025 best practices:
 * - GPT-5 with max_completion_tokens (NOT max_tokens)
 * - reasoning_effort: "high" for maximum reasoning
 * - verbosity control (low/medium/high)
 * - NO temperature parameter (deprecated in GPT-5)
 * - Structured Outputs with strict: true
 * - Manual parallel tool calls handling
 * - Support for both direct OpenAI and AI SDK patterns
 */

import OpenAI from 'openai';
import { openai } from '@ai-sdk/openai';
import { streamText, generateText, CoreMessage } from 'ai';
import type { ChatCompletionMessageParam } from 'openai/resources/chat/completions';

// ===================================================
// üìã TYPES & INTERFACES
// ===================================================

export interface AIProviderConfig {
  model?: string;
  maxCompletionTokens?: number; // ‚úÖ GPT-5 uses this (NOT maxTokens)
  reasoningEffort?: 'minimal' | 'high'; // ‚úÖ GPT-5 September 2025 feature
  verbosity?: 'low' | 'medium' | 'high'; // ‚úÖ GPT-5 September 2025 feature
  maxToolRoundtrips?: number;
  timeout?: number;
  apiKey?: string;
  enableStructuredOutputs?: boolean;
  // ‚ùå DEPRECATED: temperature (causes GPT-5 API errors)
  // ‚ùå DEPRECATED: maxTokens (use maxCompletionTokens)
}

export interface StreamingResponse {
  stream: ReadableStream;
  controller: ReadableStreamDefaultController;
}

export interface ToolCall {
  id: string;
  type: 'function';
  function: {
    name: string;
    arguments: string;
  };
}

export interface ToolResult {
  id: string;
  name: string;
  result: string;
  error?: string;
}

export type AIMessage = CoreMessage | ChatCompletionMessageParam;

// ===================================================
// üìä AI PROVIDER CLASS
// ===================================================

export class AIProvider {
  private openaiClient: OpenAI;
  private config: Required<AIProviderConfig>;

  constructor(config: AIProviderConfig = {}) {
    this.config = {
      // ‚úÖ GPT-5 is the default model (August 7, 2025 release)
      model: config.model || process.env.AI_MODEL || 'gpt-5',
      
      // ‚úÖ GPT-5 September 2025 parameters
      maxCompletionTokens: config.maxCompletionTokens ?? parseInt(process.env.MAX_COMPLETION_TOKENS || '3000'),
      reasoningEffort: config.reasoningEffort ?? (process.env.REASONING_EFFORT as 'minimal' | 'high') ?? 'high',
      verbosity: config.verbosity ?? (process.env.VERBOSITY as 'low' | 'medium' | 'high') ?? 'medium',
      
      // ‚úÖ Standard configuration
      maxToolRoundtrips: config.maxToolRoundtrips ?? 5,
      timeout: config.timeout ?? 30000,
      apiKey: config.apiKey || process.env.OPENAI_API_KEY || '',
      enableStructuredOutputs: config.enableStructuredOutputs ?? true,
      
      // ‚ùå REMOVED: temperature (deprecated in GPT-5)
      // ‚ùå REMOVED: maxTokens (use maxCompletionTokens)
    };

    if (!this.config.apiKey) {
      throw new Error('OpenAI API key not configured');
    }

    this.openaiClient = new OpenAI({
      apiKey: this.config.apiKey,
    });
  }

  // ===================================================
  // üîß DIRECT OPENAI METHODS
  // ===================================================

  /**
   * Direct OpenAI streaming with intelligent fallback for organization verification
   * GPT-5 September 2025 implementation with official parameters + fallback strategy
   */
  async streamWithOpenAI(
    messages: ChatCompletionMessageParam[],
    tools: Array<OpenAI.Chat.Completions.ChatCompletionTool> = [],
    onToolCall?: (toolCall: ToolCall) => Promise<string>
  ): Promise<{
    stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>;
    handleToolCalls: () => Promise<string>;
  }> {
    // üîÑ Smart fallback strategy for unverified organizations
    const attemptStreaming = async (useStreaming: boolean = true, fallbackModel?: string) => {
      const modelToUse = fallbackModel || this.config.model;
      const shouldStream = useStreaming && !fallbackModel; // Don't stream with fallback models
      
      return await this.openaiClient.chat.completions.create({
        // ‚úÖ GPT-5 official configuration with intelligent fallback
        model: modelToUse,
        messages,
        max_completion_tokens: this.config.maxCompletionTokens, // ‚úÖ REQUIRED for GPT-5
        stream: shouldStream,
        tools: tools.length > 0 ? tools : undefined,
        tool_choice: tools.length > 0 ? 'auto' : undefined,
        
        // ‚ùå REMOVED: response_format json_object (requires 'json' in messages, causes API errors)
        // OpenAI requirement: messages must contain 'json' to use json_object format
        
        // üöß TEMPORARILY DISABLED - SDK Compatibility Issue:
        // reasoning_effort: this.config.reasoningEffort, // ‚è≥ Not available in openai@4.104.0
        // verbosity: this.config.verbosity,              // ‚è≥ Not available in openai@4.104.0
        // TODO: Re-enable when OpenAI SDK supports GPT-5 September 2025 parameters
        
        // ‚ùå REMOVED: temperature (causes GPT-5 API errors)
        // ‚ùå REMOVED: max_tokens (use max_completion_tokens)
      });
    };

    try {
      // üöÄ First attempt: GPT-5 with streaming (preferred)
      const stream = await attemptStreaming(true);
      // Return the stream directly since it's already an AsyncIterable
      return this.handleStreamResponse(stream as AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>, messages, tools, onToolCall);
      
    } catch (error: any) {
      // üîç Handle specific organization verification errors
      if (error?.status === 400 && 
          error?.message?.includes('organization must be verified to stream')) {
        
        console.warn('‚ö†Ô∏è Organization not verified for GPT-5 streaming, falling back to non-streaming...');
        
        // üîÑ Fallback 1: GPT-5 without streaming
        try {
          const response = await attemptStreaming(false);
          return this.handleNonStreamResponse(response as OpenAI.Chat.Completions.ChatCompletion, messages, tools, onToolCall);
        } catch (fallbackError: any) {
          
          console.warn('‚ö†Ô∏è GPT-5 non-streaming failed, trying GPT-4o fallback...');
          
          // üîÑ Fallback 2: GPT-4o with streaming (most compatible)
          try {
            const fallbackResponse = await attemptStreaming(true, 'gpt-4o');
            return this.handleStreamResponse(fallbackResponse as AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>, messages, tools, onToolCall);
          } catch (finalError: any) {
            
            // üîÑ Fallback 3: GPT-4o without streaming (last resort)
            console.warn('‚ö†Ô∏è Final fallback: GPT-4o non-streaming...');
            const finalResponse = await attemptStreaming(false, 'gpt-4o');
            return this.handleNonStreamResponse(finalResponse as OpenAI.Chat.Completions.ChatCompletion, messages, tools, onToolCall);
          }
        }
      } else {
        // üö® Re-throw non-verification errors
        throw error;
      }
    }
  }

  /**
   * Handle streaming response with tool calls
   */
  private async handleStreamResponse(
    stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>,
    messages: ChatCompletionMessageParam[],
    tools: Array<OpenAI.Chat.Completions.ChatCompletionTool>,
    onToolCall?: (toolCall: ToolCall) => Promise<string>
  ): Promise<{ stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>; handleToolCalls: () => Promise<string>; }> {

    const handleToolCalls = async (): Promise<string> => {
      let fullResponse = '';
      const toolCalls: ToolCall[] = [];
      let currentToolCall: Partial<ToolCall> | null = null;

      // Collect streaming response and tool calls
      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta;
        
        // Collect text content
        if (delta?.content) {
          fullResponse += delta.content;
        }

        // Collect tool calls (manual handling for parallel calls)
        if (delta?.tool_calls) {
          for (const toolCall of delta.tool_calls) {
            if (toolCall.index !== undefined) {
              if (!toolCalls[toolCall.index]) {
                toolCalls[toolCall.index] = {
                  id: '',
                  type: 'function',
                  function: { name: '', arguments: '' }
                };
              }

              const tc = toolCalls[toolCall.index];
              if (toolCall.id) tc.id = toolCall.id;
              if (toolCall.function?.name) tc.function.name += toolCall.function.name;
              if (toolCall.function?.arguments) tc.function.arguments += toolCall.function.arguments;
            }
          }
        }
      }

      // Execute tool calls if present and onToolCall handler provided
      if (toolCalls.length > 0 && onToolCall) {
        const toolResults = await Promise.all(
          toolCalls.filter(tc => tc.id && tc.function.name).map(async (toolCall) => {
            try {
              const result = await onToolCall(toolCall);
              return { role: 'tool' as const, tool_call_id: toolCall.id, content: result };
            } catch (error) {
              return { 
                role: 'tool' as const, 
                tool_call_id: toolCall.id, 
                content: `Error: ${error instanceof Error ? error.message : 'Tool execution failed'}` 
              };
            }
          })
        );

        // Second completion with tool results
        if (toolResults.length > 0) {
          const followupMessages: ChatCompletionMessageParam[] = [
            ...messages,
            { role: 'assistant', content: fullResponse, tool_calls: toolCalls },
            ...toolResults,
          ];

          const followupResponse = await this.openaiClient.chat.completions.create({
            // ‚úÖ GPT-5 official configuration (September 2025)
            model: this.config.model,
            messages: followupMessages,
            max_completion_tokens: this.config.maxCompletionTokens, // ‚úÖ REQUIRED for GPT-5
            stream: false, // Non-streaming for tool results
            
            // üöß TEMPORARILY DISABLED - SDK Compatibility Issue:
            // reasoning_effort: this.config.reasoningEffort, // ‚è≥ Not available in openai@4.104.0
            // verbosity: this.config.verbosity,              // ‚è≥ Not available in openai@4.104.0
            // TODO: Re-enable when OpenAI SDK supports GPT-5 September 2025 parameters
            
            // ‚ùå REMOVED: temperature (causes GPT-5 API errors)
            // ‚ùå REMOVED: max_tokens (use max_completion_tokens)
          });

          return followupResponse.choices[0]?.message?.content || fullResponse;
        }
      }

      return fullResponse;
    };

    return { stream, handleToolCalls };
  }

  /**
   * Handle non-streaming response with tool calls (for unverified organizations)
   */
  private async handleNonStreamResponse(
    response: OpenAI.Chat.Completions.ChatCompletion,
    messages: ChatCompletionMessageParam[],
    tools: Array<OpenAI.Chat.Completions.ChatCompletionTool>,
    onToolCall?: (toolCall: ToolCall) => Promise<string>
  ): Promise<{ stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>; handleToolCalls: () => Promise<string>; }> {
    
    const message = response.choices[0]?.message;
    let fullResponse = message?.content || '';
    
    // Handle tool calls if present
    if (message?.tool_calls && message.tool_calls.length > 0 && onToolCall) {
      const toolResults = await Promise.all(
        message.tool_calls.map(async (toolCall) => {
          try {
            const result = await onToolCall({
              id: toolCall.id,
              type: toolCall.type,
              function: toolCall.function
            });
            return { role: 'tool' as const, tool_call_id: toolCall.id, content: result };
          } catch (error) {
            return { 
              role: 'tool' as const, 
              tool_call_id: toolCall.id, 
              content: `Error: ${error instanceof Error ? error.message : 'Tool execution failed'}` 
            };
          }
        })
      );

      // Second completion with tool results
      if (toolResults.length > 0) {
        const followupMessages: ChatCompletionMessageParam[] = [
          ...messages,
          { role: 'assistant', content: fullResponse, tool_calls: message.tool_calls },
          ...toolResults,
        ];

        const followupResponse = await this.openaiClient.chat.completions.create({
          model: this.config.model,
          messages: followupMessages,
          max_completion_tokens: this.config.maxCompletionTokens,
          stream: false,
        });

        fullResponse = followupResponse.choices[0]?.message?.content || fullResponse;
      }
    }
    
    // Create a mock stream that returns the complete response
    const mockStream = (async function* () {
      yield {
        id: response.id,
        object: 'chat.completion.chunk' as const,
        created: response.created,
        model: response.model,
        choices: [{
          index: 0,
          delta: { content: fullResponse, role: 'assistant' as const },
          finish_reason: 'stop' as const
        }]
      };
    })();

    const handleToolCalls = async () => fullResponse;

    return { stream: mockStream, handleToolCalls };
  }

  // ===================================================
  // üöÄ VERCEL AI SDK V5 METHODS
  // ===================================================

  /**
   * Vercel AI SDK v5 streamText with GPT-5 September 2025 configuration
   * Best for React integration and automatic tool handling
   */
  async streamWithVercelAI(
    messages: CoreMessage[],
    tools: Record<string, any> = {},
    systemPrompt?: string
  ) {
    return await streamText({
      model: openai(this.config.model), // ‚úÖ GPT-5 with Vercel AI SDK
      messages,
      system: systemPrompt,
      maxOutputTokens: this.config.maxCompletionTokens, // ‚úÖ Updated parameter name
      tools,
      toolChoice: Object.keys(tools).length > 0 ? 'auto' : undefined,
      // üöß TEMPORARILY DISABLED - SDK Compatibility Issue:
      // maxToolRoundtrips: this.config.maxToolRoundtrips, // ‚è≥ Not available in Vercel AI SDK v5.0.29
      // experimental_reasoningEffort: this.config.reasoningEffort, // ‚è≥ Not available in Vercel AI SDK yet
      // TODO: Re-enable when Vercel AI SDK supports these parameters
      
      // üöß ADVANCED FEATURES - May need SDK compatibility verification:
      // stopWhen: (step) => {
      //   // Stop if no tool calls in current step (conversation complete)
      //   return step.toolCalls.length === 0 && step.text.length > 0;
      // },

      // prepareStep: ({ messages, tools, toolChoice }) => ({
      //   messages: messages.slice(-10), // Keep last 10 messages for context
      //   tools,
      //   toolChoice,
      // }),
      
      // ‚ùå REMOVED: temperature (deprecated in GPT-5)
    });
  }

  /**
   * Non-streaming text generation with GPT-5 September 2025 configuration
   */
  async generateWithTools(
    messages: CoreMessage[],
    tools: Record<string, any> = {},
    systemPrompt?: string
  ) {
    return await generateText({
      model: openai(this.config.model), // ‚úÖ GPT-5 with Vercel AI SDK
      messages,
      system: systemPrompt,
      maxOutputTokens: this.config.maxCompletionTokens, // ‚úÖ Updated parameter name
      tools,
      toolChoice: Object.keys(tools).length > 0 ? 'auto' : undefined,
      // üöß TEMPORARILY DISABLED - SDK Compatibility Issue:
      // maxToolRoundtrips: this.config.maxToolRoundtrips, // ‚è≥ Not available in Vercel AI SDK v5.0.29
      // experimental_reasoningEffort: this.config.reasoningEffort, // ‚è≥ Not available in Vercel AI SDK yet
      // TODO: Re-enable when Vercel AI SDK supports these parameters
      
      // ‚ùå REMOVED: temperature (deprecated in GPT-5)
    });
  }

  // ===================================================
  // üõ†Ô∏è HELPER METHODS
  // ===================================================

  /**
   * Convert OpenAI messages to CoreMessage format
   */
  convertToCoreMessages(messages: ChatCompletionMessageParam[]): CoreMessage[] {
    return messages.map(msg => {
      if (msg.role === 'system') {
        return { role: 'system', content: msg.content as string };
      } else if (msg.role === 'user') {
        return { role: 'user', content: msg.content as string };
      } else if (msg.role === 'assistant') {
        return { role: 'assistant', content: msg.content as string };
      } else {
        return { role: 'user', content: JSON.stringify(msg) };
      }
    });
  }

  /**
   * Convert CoreMessage to OpenAI format
   */
  convertToOpenAIMessages(messages: CoreMessage[]): ChatCompletionMessageParam[] {
    return messages.map(msg => {
      const content = typeof msg.content === 'string' 
        ? msg.content 
        : Array.isArray(msg.content) 
          ? msg.content.map(part => {
              if (typeof part === 'string') return part;
              // Type-safe handling of multi-modal content parts
              if ('text' in part) return part.text || '';
              if ('image' in part) return '[Image]'; // ImagePart
              if ('file' in part) return '[File]'; // FilePart
              return '[Content]'; // Other part types
            }).join('') 
          : '';

      return {
        role: msg.role,
        content,
      } as ChatCompletionMessageParam;
    });
  }

  /**
   * Create streaming response with proper headers (2025 best practice)
   */
  createStreamResponse(stream: ReadableStream, headers: Record<string, string> = {}): Response {
    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Cache-Control': 'no-cache, no-transform',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no', // Disable Nginx buffering
        ...headers,
      },
    });
  }

  /**
   * Get model configuration info
   */
  getConfig(): Required<AIProviderConfig> {
    return { ...this.config };
  }

  /**
   * Test API connection with GPT-5 September 2025 parameters
   */
  async testConnection(): Promise<{ success: boolean; model: string; error?: string }> {
    try {
      const response = await this.openaiClient.chat.completions.create({
        // ‚úÖ GPT-5 official test configuration
        model: this.config.model,
        messages: [{ role: 'user', content: 'Test connection' }],
        max_completion_tokens: 10, // ‚úÖ REQUIRED for GPT-5
        
        // üöß TEMPORARILY DISABLED - SDK Compatibility Issue:
        // reasoning_effort: 'minimal', // ‚è≥ Not available in openai@4.104.0
        // verbosity: 'low',            // ‚è≥ Not available in openai@4.104.0
        // TODO: Re-enable when OpenAI SDK supports GPT-5 September 2025 parameters
        
        // ‚ùå REMOVED: max_tokens (use max_completion_tokens)
      });

      return {
        success: true,
        model: response.model || this.config.model,
      };
    } catch (error) {
      return {
        success: false,
        model: this.config.model,
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }
}

// ===================================================
// üèóÔ∏è FACTORY FUNCTIONS
// ===================================================

/**
 * Create AI provider with GPT-5 September 2025 configuration
 */
export function createAIProvider(config: AIProviderConfig = {}): AIProvider {
  return new AIProvider(config);
}

/**
 * Create AI provider with GPT-5 development defaults
 */
export function createDevAIProvider(): AIProvider {
  return new AIProvider({
    model: 'gpt-5', // ‚úÖ Always use GPT-5 (even in dev)
    enableStructuredOutputs: true,
    maxToolRoundtrips: 3, // Lower for dev to avoid costs
    reasoningEffort: 'minimal', // ‚úÖ Use minimal for dev to save costs
    verbosity: 'low', // ‚úÖ Shorter responses for dev
    maxCompletionTokens: 1500, // ‚úÖ Lower token limit for dev
    
    // ‚ùå REMOVED: temperature (deprecated in GPT-5)
  });
}